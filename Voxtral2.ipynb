{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrZId51StrF9s/4TM2Ah9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAMeh-ZAGhloul/Mistral-Voxtral/blob/main/Voxtral2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-HOM8yQbq6K",
        "outputId": "4810eb25-dbf9-491c-8662-a27dd10008d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'voxtral-test' already exists and is not an empty directory.\n",
            "total 24\n",
            "-rw-------  1 root root 9322 Jul 19 07:00 nohup.out\n",
            "drwxr-xr-x  1 root root 4096 Jul 16 13:48 sample_data\n",
            "drwxr-xr-x 16 root root 4096 Jul 19 06:23 vllm\n",
            "drwxr-xr-x  3 root root 4096 Jul 19 06:19 voxtral-test\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/coezbek/voxtral-test\n",
        "\n",
        "! git clone https://github.com/coezbek/voxtral-test\n",
        "! ls -l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! uv pip install -q -U \"vllm[audio]\" --torch-backend=cu126 --extra-index-url https://wheels.vllm.ai/nightly\n",
        "! uv pip install -q -U mistral_common\\[audio\\] \"numpy<2.3\""
      ],
      "metadata": {
        "id": "e83BF2l3b7K1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python -c \"import mistral_common; print(mistral_common.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3dCNvq9e2FC",
        "outputId": "3ee365f4-76de-454b-b910-5bf98dbda7dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ljb0vjj0Hd",
        "outputId": "1ef4f465-ad07-4a03-9bbe-40ae8501ba8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 19 07:00:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nohup vllm serve mistralai/Voxtral-Mini-3B-2507 \\\n",
        "  --port 8333 \\\n",
        "  --tokenizer-mode mistral \\\n",
        "  --config-format mistral \\\n",
        "  --load-format mistral \\\n",
        "  --max-model-len 8192 \\\n",
        "  --gpu-memory-utilization 0.9 \\\n",
        "  --dtype float16 &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhYyUuvucIah",
        "outputId": "2d086dce-fde7-4206-d561-a1511269679f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 30 nohup.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9KrhMFOgA4E",
        "outputId": "e19d05b7-6e3c-4d70-f713-22c76db41a37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/responses, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/responses/{response_id}, Methods: GET\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/completions, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/embeddings, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /pooling, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /classify, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /score, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/score, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /rerank, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v1/rerank, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /v2/rerank, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /scale_elastic_ep, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /is_scaling_elastic_ep, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /invocations, Methods: POST\n",
            "INFO 07-19 07:02:58 [launcher.py:37] Route: /metrics, Methods: GET\n",
            "INFO:     Started server process [14509]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR 07-19 07:06:20 [client.py:306] RuntimeError('Engine process (pid 14752) died.')\r\n",
            "ERROR 07-19 07:06:20 [client.py:306] NoneType: None\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [14509]\n",
            "/usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/vllm-project/vllm\n",
        "! cd vllm && pip install -e .  # builds the native extension\n",
        "\n",
        "! python examples/offline_inference/audio_language.py --num-audios 2 --model-type voxtral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnGQMqzbpejL",
        "outputId": "060ea5a1-ee1e-4965-98be-bbc5326335d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'vllm' already exists and is not an empty directory.\n",
            "Obtaining file:///content/vllm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd voxtral-test && pwd && uv run transcribe.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOu4kVQCcvMI",
        "outputId": "e5b23eb6-341c-4edc-e40f-862e9afb80d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/voxtral-test\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/voxtral-test/transcribe.py\", line 1, in <module>\n",
            "    from mistral_common.protocol.transcription.request import TranscriptionRequest\n",
            "ModuleNotFoundError: No module named 'mistral_common'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd voxtral-test && pwd && uv run streaming.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFTcp98udISu",
        "outputId": "c6713dbd-7459-4311-ea0c-3d2e51151e60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/voxtral-test\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/voxtral-test/streaming.py\", line 1, in <module>\n",
            "    from mistral_common.protocol.transcription.request import TranscriptionRequest\n",
            "ModuleNotFoundError: No module named 'mistral_common'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cL5LJwPdNTO"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}