{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNIr1CJgPv6LOQGUFg7mxrt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAMeh-ZAGhloul/Mistral-Voxtral/blob/main/Voxtral2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-HOM8yQbq6K",
        "outputId": "48a04223-a354-4e93-d4bd-e30f90317eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'voxtral-test' already exists and is not an empty directory.\n",
            "total 44\n",
            "-rw-------  1 root root 24742 Jul 19 07:06 nohup.out\n",
            "drwxr-xr-x  1 root root  4096 Jul 16 13:48 sample_data\n",
            "drwxr-xr-x 18 root root  4096 Jul 19 07:15 vllm\n",
            "drwxr-xr-x  3 root root  4096 Jul 19 06:19 voxtral-test\n"
          ]
        }
      ],
      "source": [
        "#https://github.com/coezbek/voxtral-test\n",
        "\n",
        "! git clone https://github.com/coezbek/voxtral-test\n",
        "! ls -l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! uv pip install -q -U \"vllm[audio]\" --torch-backend=cu126 --extra-index-url https://wheels.vllm.ai/nightly\n",
        "! uv pip install -q -U mistral_common\\[audio\\] \"numpy<2.3\""
      ],
      "metadata": {
        "id": "e83BF2l3b7K1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python -c \"import mistral_common; print(mistral_common.__version__)\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3dCNvq9e2FC",
        "outputId": "40e7ad8a-831a-4057-8765-d99a6b5c52bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ljb0vjj0Hd",
        "outputId": "ef91ac02-8438-4c55-ae9d-c341177b1d69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 19 07:32:51 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/vllm-project/vllm\n",
        "! cd vllm && pip install -q -e .  # builds the native extension\n",
        "\n",
        "! python examples/offline_inference/audio_language.py --num-audios 2 --model-type voxtral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnGQMqzbpejL",
        "outputId": "c6a5c1b6-aa80-44c7-e664-03a066412d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'vllm' already exists and is not an empty directory.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nohup vllm serve mistralai/Voxtral-Mini-3B-2507 \\\n",
        "  --port 8333 \\\n",
        "  --tokenizer-mode mistral \\\n",
        "  --config-format mistral \\\n",
        "  --load-format mistral \\\n",
        "  --max-model-len 8192 \\\n",
        "  --gpu-memory-utilization 0.9 \\\n",
        "  --dtype float16 &"
      ],
      "metadata": {
        "id": "FhYyUuvucIah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 10 nohup.out"
      ],
      "metadata": {
        "id": "Y9KrhMFOgA4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd voxtral-test && pwd && uv run transcribe.py"
      ],
      "metadata": {
        "id": "xOu4kVQCcvMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cd voxtral-test && pwd && uv run streaming.py"
      ],
      "metadata": {
        "id": "TFTcp98udISu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cL5LJwPdNTO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}